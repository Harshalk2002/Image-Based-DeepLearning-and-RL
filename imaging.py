# -*- coding: utf-8 -*-
"""Imaging.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1j_zqMNkk557K2t-PtROKgDYiFCJVod9e

# Object Detection
"""

# Commented out IPython magic to ensure Python compatibility.
# Step 1: Install YOLOv5
!git clone https://github.com/ultralytics/yolov5
# %cd yolov5
!pip install -r requirements.txt

# Step 2: Load YOLOv5 Model
import torch
device = 'cuda' if torch.cuda.is_available() else 'cpu'
model = torch.hub.load('ultralytics/yolov5', 'yolov5s', source='github', force_reload=True).to(device)
model.eval()

# Step 3: Upload images
from google.colab import files
uploaded = files.upload()

# Step 4: Save uploads to new folder
import os
import shutil

upload_dir = '/content/yolov5/uploads'
os.makedirs(upload_dir, exist_ok=True)

for filename in uploaded.keys():
    shutil.move(filename, os.path.join(upload_dir, filename))

# Step 5: Perform Detection on All Uploaded Images
uploaded_images = [os.path.join(upload_dir, f) for f in os.listdir(upload_dir)
                   if f.lower().endswith(('.jpg', '.jpeg', '.png'))]

results = model(uploaded_images)  # <<< Detect ALL images at once as batch
results.save()

# Step 6: Display Detected Images
from IPython.display import Image, display
import glob

# Find the latest exp folder (where YOLOv5 saves detections)
exp_folder = sorted(glob.glob('runs/detect/exp*'))[-1]

# Display each detected image
for img_path in os.listdir(exp_folder):
    if img_path.lower().endswith(('.jpg', '.jpeg', '.png')):
        display(Image(filename=os.path.join(exp_folder, img_path)))

# Step 7: Zip detected images
!zip -r detected_images.zip {exp_folder}

# Step 8: Download the zip
files.download('detected_images.zip')

"""# Image Captioning"""

#Libraries
import numpy as np
import tensorflow as tf
from tensorflow.keras.applications import ResNet50
from tensorflow.keras.preprocessing.image import load_img, img_to_array
from tensorflow.keras.preprocessing.sequence import pad_sequences
from tensorflow.keras.models import Model
from tensorflow.keras.layers import Input, LSTM, Embedding, Dense, Add, Layer

import os
import json
import pickle
import zipfile
import requests
from io import BytesIO
import shutil
import json
import requests
import zipfile

from google.colab import files
from PIL import Image
from IPython.display import display

uploaded = files.upload()

upload_dir = '/Users/pamelaalvarado/Downloads/runs'
os.makedirs(upload_dir, exist_ok=True)
for fname in uploaded.keys():
    shutil.move(fname, os.path.join(upload_dir, fname))

uploaded_images = [
    os.path.join(upload_dir, f)
    for f in os.listdir(upload_dir)
    if f.lower().endswith(('.jpg', '.jpeg', '.png'))
]

image_ids = list(range(len(uploaded_images)))
image_paths = {i: uploaded_images[i] for i in image_ids}

#Create Image Captions of Images
#OpenAI. “Captions for Five Images.” ChatGPT, 25 Apr. 2025, chat.openai.com. Large language model output.

image_captions = [
    [
        "<start> a close-up portrait of a tan and white dog with perked ears looking at the camera <end>",
        "<start> a fox-like dog sitting on a stone patio gazing forward <end>",
        "<start> a golden canine with erect ears and a black nose on paving stones <end>",
        "<start> a shiba inu style dog with warm russet fur staring at the lens <end>",
        "<start> a medium-sized dog with white chest and alert expression on tiled ground <end>"
    ],
    [
        "<start> a ruby-throated hummingbird hovering in front of a green background <end>",
        "<start> a small hummingbird with iridescent red throat and outstretched wings mid-flight <end>",
        "<start> a tiny bird with long beak flapping its wings in midair <end>",
        "<start> a close-up of a hovering hummingbird showing blurred wings <end>",
        "<start> a male hummingbird suspended in flight against a soft green backdrop <end>"
    ],
    [
        "<start> an orange Range Rover Evoque parked on grass at a car show <end>",
        "<start> a vivid orange SUV displayed beside a purple crossover under white tents <end>",
        "<start> two sporty SUVs, one orange and one purple, lined up on a manicured lawn <end>",
        "<start> a bright orange luxury crossover with polished wheels on the grass <end>",
        "<start> an orange Range Rover model exhibited outdoors with onlookers <end>"
    ],
    [
        "<start> a sleek silver Nissan GT-R parked on a city street in front of columns <end>",
        "<start> a metallic silver sports car with black wheels parked on asphalt <end>",
        "<start> a high-performance coupe under leafy trees on a downtown road <end>",
        "<start> a low-slung silver GT-R near white arched architecture <end>",
        "<start> a luxury performance vehicle parked by a plaza with pillars <end>"
    ],
    [
        "<start> a black-and-white ruffed lemur sitting on a tree branch with a fluffy collar <end>",
        "<start> a lemur with tufted ears lounging against a log in its enclosure <end>",
        "<start> a primate with monochrome fur perched on a branch looking away <end>",
        "<start> a relaxed black-and-white lemur draped over a wooden limb <end>",
        "<start> a fluffy-collared lemur casually resting on a tree branch <end>"
    ]
]

# Parameters
MAX_LENGTH = 34
VOCAB_SIZE = 5000
EMBED_DIM = 256
UNITS = 512
BATCH_SIZE = 64
EPOCHS = 30

# Tokenizer
all_captions = [c for caps in image_captions for c in caps]
tokenizer = tf.keras.preprocessing.text.Tokenizer(num_words=VOCAB_SIZE, oov_token='<unk>')
tokenizer.fit_on_texts(all_captions)
word_index = tokenizer.word_index
index_word = {v: k for k, v in word_index.items()}

with open('tokenizer.pkl', 'wb') as f:
    pickle.dump(tokenizer, f)

def load_image(img_path):
    img = load_img(img_path, target_size=(224, 224))
    img = img_to_array(img)
    img = tf.keras.applications.resnet50.preprocess_input(img)
    return img

def build_cnn_encoder():
    base_model = ResNet50(include_top=False, weights='imagenet', pooling='avg')
    input_img = Input(shape=(224, 224, 3))
    features = base_model(input_img)
    model = Model(inputs=input_img, outputs=features)
    return model

encoder = build_cnn_encoder()

# RNN Decoder
class RNNDecoder(tf.keras.Model):
    def __init__(self, embedding_dim, units, vocab_size):
        super().__init__()
        self.units = units
        self.embedding = Embedding(vocab_size, embedding_dim)
        self.lstm = LSTM(units, return_sequences=True, return_state=True)
        self.fc1 = Dense(units)
        self.fc2 = Dense(vocab_size)

    def call(self, features, captions):
        embeddings = self.embedding(captions)
        features = tf.expand_dims(features, 1)
        features = tf.tile(features, [1, tf.shape(captions)[1], 1])
        x = tf.concat([features, embeddings], axis=-1)
        output, _, _ = self.lstm(x)
        x = self.fc1(output)
        x = self.fc2(x)
        return x

decoder = RNNDecoder(EMBED_DIM, UNITS, VOCAB_SIZE)

image_ids = list(range(len(image_captions)))
encoder = build_cnn_encoder()
image_features = {}
for img_id in image_ids:
    path = image_paths[img_id]
    img = load_image(path)
    img = np.expand_dims(img, axis=0)
    image_features[img_id] = encoder.predict(img, verbose=0)[0]

# Prepare Input-Output Pairs
input_seqs, target_seqs, img_feats = [], [], []
for img_id in image_ids:
    for cap in image_captions[img_id]:
        seq = tokenizer.texts_to_sequences([cap])[0]
        for i in range(1, len(seq)):
            in_seq, out_seq = seq[:i], seq[i]
            in_seq = pad_sequences([in_seq], maxlen=MAX_LENGTH)[0]
            input_seqs.append(in_seq)
            target_seqs.append(out_seq)
            img_feats.append(image_features[img_id])

input_seqs = np.array(input_seqs)
target_seqs = np.array(target_seqs)
img_feats = np.array(img_feats)

loss_object = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)
optimizer = tf.keras.optimizers.Adam()

# Training step
@tf.function
def train_step(img_tensor, input_seq, target):
    with tf.GradientTape() as tape:
        predictions = decoder(img_tensor, input_seq)
        loss = loss_object(target, predictions[:, -1, :])
    grads = tape.gradient(loss, decoder.trainable_variables)
    optimizer.apply_gradients(zip(grads, decoder.trainable_variables))
    return loss

# Training loop
for epoch in range(EPOCHS):
    total_loss = 0
    num_batches = len(input_seqs) // BATCH_SIZE
    for i in range(0, len(input_seqs), BATCH_SIZE):
        img_batch = img_feats[i:i+BATCH_SIZE]
        seq_batch = input_seqs[i:i+BATCH_SIZE]
        tgt_batch = target_seqs[i:i+BATCH_SIZE]
        loss = train_step(img_batch, seq_batch, tgt_batch)
        total_loss += loss
    avg_loss = total_loss / num_batches if num_batches > 0 else total_loss
    print(f"Epoch {epoch+1}/{EPOCHS}, Avg Loss: {avg_loss:.4f}")

decoder.save_weights("caption_decoder.weights.h5")
print("Saved decoder weights")

def generate_caption(image_path, encoder, decoder, tokenizer, max_length=MAX_LENGTH):
    img = load_image(image_path)
    img = np.expand_dims(img, axis=0)
    feature = encoder.predict(img, verbose=0)

    input_text = ['<start>']

    for _ in range(max_length):
        seq = tokenizer.texts_to_sequences([' '.join(input_text)])[0]
        seq = pad_sequences([seq], maxlen=max_length)

        predictions = decoder(feature, seq)
        predicted_id = tf.argmax(predictions[:, -1, :], axis=-1).numpy()[0]
        predicted_word = index_word.get(predicted_id, '<unk>')

        if predicted_word == '<end>':
            break

        input_text.append(predicted_word)

    return ' '.join(input_text[1:])

decoder.load_weights('caption_decoder.weights.h5')

images = 5

for img_id in range(images):
    test_image_path = image_paths[img_id]
    caption = generate_caption(test_image_path, encoder, decoder, tokenizer)
    print(f"Image ID: {img_id}")
    print(f"Generated Caption: {caption}")
    ground_truth = image_captions[img_id] if img_id < len(image_captions) else ["No captions available"]
    print(f"Ground Truth Captions:")
    for i, cap in enumerate(ground_truth, 1):
        print(f"    {i}) {cap}")
    print("-" * 50)